# Neural-Network-Numpy
Implemented a NN from scratch using the Numpy library.

## Activation Functions Implemented:

### ReLU
### Linear
### Sigmoid
### TanH
### Softmax

## Loss Function Employed: Cross Entropy Loss
### Optimization Using SGD

### Hyper-Parmas: Epochs, Layer sizes, Activation Function Used, Batch Size, Learning Rate

Comparisons are also made with SkLearn's NN, and losses and testing accuracies are compared

### Dataset used: MNIST
