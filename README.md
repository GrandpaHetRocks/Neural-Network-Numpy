# Neural-Network-Numpy
Implemented a NN from scratch using the Numpy library.

## Activation Functions Implemented:

### ReLU
### Linear
### Sigmoid
### TanH
### Softmax

## Loss Function Employed: Cross Entropy Loss

### Hyper-Parmas: Epochs, Layer sizes, Activation Function Used

Comparisons also made with SkLearn's NN, and lossed and testing accuracies are compared

### Dataset used: MNIST
